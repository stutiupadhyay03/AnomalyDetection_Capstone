{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03b4d060",
   "metadata": {},
   "source": [
    "# Avenue Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2ae74b",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook contains the final implementation of the anomaly detection system on the Avenue Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9940e53",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import dependencies and configure the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "096d166e-edc5-4ec9-8a3f-67b87176babf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21 test videos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames: 100%|████████████████████████| 21/21 [00:13<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === SETTINGS ===\n",
    "video_dir = \"datasets/Avenue Dataset/testing_videos\"\n",
    "output_dir = \"datasets/Avenue Dataset/frames/test\"\n",
    "frame_ext = \".jpg\"\n",
    "\n",
    "# Make sure output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# List all .avi video files\n",
    "video_files = [f for f in os.listdir(video_dir) if f.endswith('.avi')]\n",
    "\n",
    "print(f\"Found {len(video_files)} test videos.\")\n",
    "\n",
    "# === FRAME EXTRACTION ===\n",
    "for vid in tqdm(video_files, desc=\"Extracting frames\"):\n",
    "    video_path = os.path.join(video_dir, vid)\n",
    "    vid_name = os.path.splitext(vid)[0]\n",
    "    vid_output_dir = os.path.join(output_dir, vid_name)\n",
    "    os.makedirs(vid_output_dir, exist_ok=True)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_idx = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_path = os.path.join(vid_output_dir, f\"{vid_name}_frame{frame_idx:04d}{frame_ext}\")\n",
    "        cv2.imwrite(frame_path, frame)\n",
    "        frame_idx += 1\n",
    "    cap.release()\n",
    "\n",
    "print(\"Frame extraction complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afc7d96",
   "metadata": {},
   "source": [
    "## Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c32bf0-fbf7-411e-b239-7aa3e3b78122",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io\n",
    "from glob import glob\n",
    "\n",
    "def load_avenue_gt_mat(gt_folder):\n",
    "    gt_frame_map = {}\n",
    "\n",
    "    mat_files = sorted(glob(os.path.join(gt_folder, 'vol*.mat')))\n",
    "    for idx, mat_file in enumerate(mat_files):\n",
    "        mat_data = scipy.io.loadmat(mat_file)\n",
    "        video_id = f\"{idx+1:02d}\"  # e.g., '01', '02'\n",
    "        anomalies = mat_data.get('gt')  # shape (1, n) or (n, 1)\n",
    "\n",
    "        if anomalies is not None:\n",
    "            anomaly_frames = anomalies[0] if anomalies.shape[0] == 1 else anomalies[:,0]\n",
    "            gt_frame_map[video_id] = set(anomaly_frames.tolist())\n",
    "        else:\n",
    "            gt_frame_map[video_id] = set()\n",
    "\n",
    "    return gt_frame_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "639c4fc6-c414-435a-875c-c98138dc6fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "import re\n",
    "\n",
    "class AvenueFrameDataset(Dataset):\n",
    "    def __init__(self, frame_dir, label_dir, gt_frame_map, transform=None):\n",
    "        self.samples = []\n",
    "        self.transform = transform\n",
    "\n",
    "        video_dirs = sorted(os.listdir(frame_dir))\n",
    "        for video in video_dirs:\n",
    "            video_id = video  # e.g., \"01\", \"02\", ...\n",
    "            full_video_dir = os.path.join(frame_dir, video)\n",
    "            frames = sorted(os.listdir(full_video_dir))\n",
    "\n",
    "            for frame_name in frames:\n",
    "                frame_path = os.path.join(full_video_dir, frame_name)\n",
    "                label_path = os.path.join(label_dir, video, frame_name.replace('.jpg', '.txt'))\n",
    "\n",
    "                #frame_num = int(frame_name.split('frame')[-1].split('.')[0])\n",
    "                match = re.search(r'frame(\\d+)', frame_name)\n",
    "                if not match:\n",
    "                    continue  # Skip files that don't match expected naming format\n",
    "                    \n",
    "                frame_num = int(match.group(1))\n",
    "                gt_label = 1 if frame_num in gt_frame_map.get(video_id, set()) else 0\n",
    "\n",
    "                self.samples.append({\n",
    "                    'frame': frame_path,\n",
    "                    'label_file': label_path,\n",
    "                    'gt_label': gt_label,\n",
    "                    'video': video_id,\n",
    "                    'frame_num': frame_num\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.samples[idx]\n",
    "\n",
    "        # Load image\n",
    "        img = Image.open(item['frame']).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        else:\n",
    "            img = transforms.ToTensor()(img)\n",
    "\n",
    "        # Load label file (list of detected objects)\n",
    "        label_data = []\n",
    "        if os.path.exists(item['label_file']):\n",
    "            with open(item['label_file'], 'r') as f:\n",
    "                label_data = f.read().splitlines()\n",
    "\n",
    "        return {\n",
    "            'image': img,\n",
    "            'objects': label_data,\n",
    "            'gt_label': item['gt_label'],\n",
    "            'video': item['video'],\n",
    "            'frame_num': item['frame_num']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f110dd60-67af-4760-919a-8cbd63e6b13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_frame_anomalies(object_labels):\n",
    "    counts = {'bicycle': 0, 'paper': 0, 'kid': 0}\n",
    "    anomaly_labels = []\n",
    "\n",
    "    for label in object_labels:\n",
    "        parts = label.split()\n",
    "        class_id = int(parts[0])\n",
    "\n",
    "        if class_id == 1:  # bicycle\n",
    "            counts['bicycle'] += 1\n",
    "        elif class_id == 2:  # paper\n",
    "            counts['paper'] += 1\n",
    "        elif class_id == 3:  # kid\n",
    "            counts['kid'] += 1\n",
    "\n",
    "    if counts['bicycle'] > 0:\n",
    "        anomaly_labels.append(\"abnormal object\")\n",
    "    if counts['paper'] > 0:\n",
    "        anomaly_labels.append(\"possible loitering\")\n",
    "    if counts['kid'] >= 2:\n",
    "        anomaly_labels.append(\"unusual presence (kids)\")\n",
    "\n",
    "    if not anomaly_labels:\n",
    "        anomaly_labels.append(\"normal\")\n",
    "\n",
    "    return anomaly_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ceea03f6-e31a-4409-884c-89d1ff306691",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test videos: 100%|███████████████████| 21/21 [00:00<00:00, 21.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anomaly labels saved to avenue_anomaly_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Same rule-based function as earlier\n",
    "def classify_frame_anomalies(object_labels):\n",
    "    counts = {'bicycle': 0, 'paper': 0, 'kid': 0}\n",
    "    anomaly_labels = []\n",
    "\n",
    "    for label in object_labels:\n",
    "        parts = label.split()\n",
    "        class_id = int(parts[0])\n",
    "\n",
    "        if class_id == 1:\n",
    "            counts['bicycle'] += 1\n",
    "        elif class_id == 2:\n",
    "            counts['paper'] += 1\n",
    "        elif class_id == 3:\n",
    "            counts['kid'] += 1\n",
    "\n",
    "    if counts['bicycle'] > 0:\n",
    "        anomaly_labels.append(\"abnormal object\")\n",
    "    if counts['paper'] > 0:\n",
    "        anomaly_labels.append(\"possible loitering\")\n",
    "    if counts['kid'] >= 2:\n",
    "        anomaly_labels.append(\"unusual presence (kids)\")\n",
    "\n",
    "    if not anomaly_labels:\n",
    "        anomaly_labels.append(\"normal\")\n",
    "\n",
    "    return anomaly_labels\n",
    "\n",
    "# Paths\n",
    "label_dir = \"datasets/Avenue Dataset/labels/test\"\n",
    "frame_dir = \"datasets/Avenue Dataset/frames/test\"\n",
    "\n",
    "# Output\n",
    "output_csv = \"avenue_anomaly_labels.csv\"\n",
    "results = []\n",
    "\n",
    "# Loop through videos\n",
    "video_folders = sorted(os.listdir(frame_dir))\n",
    "for video in tqdm(video_folders, desc=\"Processing test videos\"):\n",
    "    video_label_path = os.path.join(label_dir, video)\n",
    "    video_frame_path = os.path.join(frame_dir, video)\n",
    "    \n",
    "    #for frame_file in sorted(os.listdir(video_frame_path)):\n",
    "        #frame_num = int(frame_file.split(\"frame\")[-1].split(\".\")[0])\n",
    "    for frame_file in sorted(f for f in os.listdir(video_frame_path) if f.endswith(\".jpg\")):\n",
    "        try:\n",
    "            frame_num = int(frame_file.split(\"frame\")[-1].split(\".\")[0])\n",
    "        except ValueError:\n",
    "            print(f\"Skipping malformed filename: {frame_file}\")\n",
    "            continue\n",
    "\n",
    "        label_file = frame_file.replace(\".jpg\", \".txt\")\n",
    "        label_path = os.path.join(video_label_path, label_file)\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "\n",
    "        with open(label_path, 'r') as f:\n",
    "            object_labels = f.readlines()\n",
    "        \n",
    "        anomalies = classify_frame_anomalies(object_labels)\n",
    "        results.append([video, frame_num, \";\".join(anomalies)])\n",
    "\n",
    "# Save to CSV\n",
    "with open(output_csv, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"video\", \"frame\", \"anomaly_label\"])\n",
    "    writer.writerows(results)\n",
    "\n",
    "print(f\"\\nAnomaly labels saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4678a8cb-dfae-4219-9657-699b2e64802f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 21/21 [00:01<00:00, 20.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly labels saved to ave_anomaly_labels.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.io import loadmat\n",
    "\n",
    "# Paths\n",
    "frame_dir = \"datasets/Avenue Dataset/frames/test\"\n",
    "label_dir = \"datasets/Avenue Dataset/labels/test\"\n",
    "gt_mat_dir = \"datasets/Avenue Dataset/testing_vol\"\n",
    "\n",
    "# COCO class map used in YOLOv8\n",
    "class_map = {\n",
    "    0: 'person',\n",
    "    1: 'bicycle',\n",
    "    25: 'backpack',\n",
    "    26: 'umbrella',\n",
    "    27: 'handbag',\n",
    "    56: 'chair',\n",
    "    73: 'laptop',\n",
    "    74: 'mouse',\n",
    "    75: 'remote',\n",
    "    76: 'keyboard',\n",
    "    77: 'cell phone',\n",
    "    78: 'microwave',\n",
    "    79: 'oven',\n",
    "    80: 'toaster',\n",
    "    81: 'sink',\n",
    "    82: 'refrigerator',\n",
    "    83: 'book',\n",
    "    84: 'clock',\n",
    "    85: 'vase',\n",
    "    86: 'scissors',\n",
    "    87: 'teddy bear',\n",
    "    88: 'hair drier',\n",
    "    89: 'toothbrush',\n",
    "    67: 'cell phone',\n",
    "    69: 'paper',\n",
    "    70: 'bag',\n",
    "    71: 'kid'  \n",
    "}\n",
    "\n",
    "# Load GT anomaly frames\n",
    "def load_avenue_gt():\n",
    "    gt_map = {}\n",
    "    for i in range(1, 22):  # video01 to video21\n",
    "        vid = str(i).zfill(2)\n",
    "        mat_path = os.path.join(gt_mat_dir, f\"vol{vid}.mat\")\n",
    "        mat = loadmat(mat_path)\n",
    "        anomaly_frames = set(mat['vol'].flatten().tolist())\n",
    "        gt_map[vid] = anomaly_frames\n",
    "    return gt_map\n",
    "\n",
    "gt_map = load_avenue_gt()\n",
    "\n",
    "# Rule-based labeling\n",
    "def detect_anomaly(objects, frame_num, video_id):\n",
    "    class_ids = [int(line.split()[0]) for line in objects]\n",
    "    obj_names = [class_map.get(cid, \"unknown\") for cid in class_ids]\n",
    "\n",
    "    if \"bicycle\" in obj_names:\n",
    "        return \"abnormal object\"\n",
    "    elif \"paper\" in obj_names or \"bag\" in obj_names:\n",
    "        return \"possible loitering\"\n",
    "    elif obj_names.count(\"kid\") > 1:\n",
    "        return \"unusual presence\"\n",
    "    elif frame_num in gt_map.get(video_id, set()):\n",
    "        return \"unusual action\"\n",
    "    else:\n",
    "        return \"normal\"\n",
    "\n",
    "# Process all frames\n",
    "rows = []\n",
    "\n",
    "for video in tqdm(sorted(os.listdir(frame_dir))):\n",
    "    video_id = video.zfill(2)\n",
    "    frame_path = os.path.join(frame_dir, video)\n",
    "    label_path = os.path.join(label_dir, video)\n",
    "\n",
    "    if not os.path.isdir(frame_path):\n",
    "        continue\n",
    "\n",
    "    for fname in sorted(os.listdir(frame_path)):\n",
    "        if not fname.endswith(\".jpg\"):\n",
    "            continue\n",
    "\n",
    "        frame_num = int(fname.split(\"frame\")[-1].split(\".\")[0])\n",
    "        label_file = fname.replace(\".jpg\", \".txt\")\n",
    "        label_file_path = os.path.join(label_path, label_file)\n",
    "\n",
    "        if os.path.exists(label_file_path):\n",
    "            with open(label_file_path, 'r') as f:\n",
    "                objects = f.read().strip().split('\\n')\n",
    "        else:\n",
    "            objects = []\n",
    "\n",
    "        label = detect_anomaly(objects, frame_num, video_id)\n",
    "        rows.append({\n",
    "            \"video\": int(video),\n",
    "            \"frame\": frame_num,\n",
    "            \"anomaly_label\": label\n",
    "        })\n",
    "\n",
    "# Save CSV\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"ave_anomaly_labels.csv\", index=False)\n",
    "print(\"Anomaly labels saved to ave_anomaly_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad48ab73-6ecd-4c56-a7f5-9ad47ac66c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class AvenueAnomalyDataset(Dataset):\n",
    "    def __init__(self, csv_path, frame_root, transform=None):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        self.frame_root = frame_root\n",
    "        self.transform = transform\n",
    "\n",
    "        self.label2idx = {\n",
    "            \"normal\": 0,\n",
    "            \"unusual action\": 1,\n",
    "            \"abnormal object\": 2\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        video_id = str(row['video']).zfill(2)\n",
    "        frame_num = int(row['frame'])\n",
    "\n",
    "        frame_path = os.path.join(self.frame_root, video_id, f\"{video_id}_frame{frame_num:04d}.jpg\")\n",
    "        image = Image.open(frame_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label_str = row['anomaly_label']\n",
    "        label = self.label2idx.get(label_str, 0)  # fallback to \"normal\"\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea38e09-6b3c-4023-acdc-58722ee59b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "class AnomalyClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.base = resnet18(weights=\"IMAGENET1K_V1\")\n",
    "        self.base.fc = nn.Linear(self.base.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ed42008-38a0-4bd1-b749-840b8993b059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16330025-3535-4496-8d1f-df5befffc210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class AnomalyClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(AnomalyClassifier, self).__init__()\n",
    "        self.features = models.resnet18(weights=None)\n",
    "        self.features.fc = nn.Identity()  # Remove original classifier\n",
    "        self.fc = nn.Linear(512, num_classes)  # Custom final layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38103ac2-c349-4b88-8360-3b68d72a6203",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvenueAnomalyDataset(Dataset):\n",
    "    def __init__(self, dataframe, frame_root, transform=None):\n",
    "        self.data = dataframe.reset_index(drop=True)\n",
    "        self.frame_root = frame_root\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        vid = str(row['video']).zfill(2)\n",
    "        frame_num = int(row['frame'])\n",
    "        label_str = row['anomaly_label']\n",
    "        label = LABEL2IDX.get(label_str, 0)\n",
    "        frame_path = os.path.join(self.frame_root, vid, f\"{vid}_frame{frame_num:04d}.jpg\")\n",
    "        image = Image.open(frame_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a755314-34f7-43c6-a03a-60420b953026",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.base = models.resnet34(weights=\"IMAGENET1K_V1\")\n",
    "        self.base.fc = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(self.base.fc.in_features, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15621c42-7e08-448c-9e8d-192460bf0bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            preds = outputs.argmax(1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd8811a",
   "metadata": {},
   "source": [
    "## Inference & Streamlit UI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848eaf2-40e4-42b0-bd84-fbb8e7ba5a1d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b8aadd9-f8cc-4426-9b65-bce1ea8b2a4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (8.3.111)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (3.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (1.15.2)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: pandas>=1.1.4 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (2.2.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (0.13.2)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from ultralytics) (2.0.14)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/anomaly_env/lib/python3.10/site-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ultralytics opencv-python pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "542be465-d8f5-47df-8e2b-af238bd31570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: 01\n",
      "Frame #: 100\n",
      "GT Anomalous Frame?: 0\n",
      "Detected Objects: ['0 0.510281 0.485244 0.069270 0.231149', '0 0.774360 0.494346 0.056948 0.348298', '0 0.865978 0.474980 0.041805 0.388346', '0 0.392722 0.534759 0.037895 0.163697', '26 0.102191 0.807837 0.124924 0.194103', '0 0.034845 0.708353 0.069130 0.316734', '0 0.911424 0.399653 0.035667 0.120897']\n"
     ]
    }
   ],
   "source": [
    "gt_map = load_avenue_gt_mat(\"datasets/Avenue Dataset/testing_vol\")\n",
    "\n",
    "dataset = AvenueFrameDataset(\n",
    "    frame_dir=\"datasets/Avenue Dataset/frames/test\",\n",
    "    label_dir=\"datasets/Avenue Dataset/labels/test\",  \n",
    "    gt_frame_map=gt_map\n",
    ")\n",
    "\n",
    "sample = dataset[100]\n",
    "print(\"Video:\", sample['video'])\n",
    "print(\"Frame #:\", sample['frame_num'])\n",
    "print(\"GT Anomalous Frame?:\", sample['gt_label'])\n",
    "print(\"Detected Objects:\", sample['objects'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ffe87ae-6c13-4b4b-a051-7889fb8595c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal']\n"
     ]
    }
   ],
   "source": [
    "sample_labels = ['0 0.510281 0.485244 0.069270 0.231149',\n",
    "                 '0 0.774360 0.494346 0.056948 0.348298',\n",
    "                 '0 0.865978 0.474980 0.041805 0.388346',\n",
    "                 '0 0.392722 0.534759 0.037895 0.163697',\n",
    "                 '26 0.102191 0.807837 0.124924 0.194103',\n",
    "                 '0 0.034845 0.708353 0.069130 0.316734',\n",
    "                 '0 0.911424 0.399653 0.035667 0.120897']\n",
    "\n",
    "print(classify_frame_anomalies(sample_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "101269ba-5398-4255-bd7f-aba30588b895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {'normal': 0, 'unusual action': 1, 'abnormal object': 2}\n"
     ]
    }
   ],
   "source": [
    "# Load the frame-level labels CSV\n",
    "df = pd.read_csv(\"ave_anomaly_labels.csv\")  # or the path you saved it as\n",
    "df['anomaly_label'] = df['anomaly_label'].astype(str)\n",
    "\n",
    "# Encode anomaly labels to integers\n",
    "label2idx = {label: idx for idx, label in enumerate(df['anomaly_label'].unique())}\n",
    "idx2label = {v: k for k, v in label2idx.items()}\n",
    "df['label'] = df['anomaly_label'].map(label2idx)\n",
    "\n",
    "print(\"Labels:\", label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0856f636-29cf-4208-850f-1e3676244d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 68.7659, Accuracy: 93.51%\n"
     ]
    }
   ],
   "source": [
    "model = AnomalyClassifier(num_classes=3).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15275dbe-ba90-4d33-ba6f-61fa9854b66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "         normal       0.99      0.99      0.99      2071\n",
      " unusual action       0.98      0.98      0.98       951\n",
      "abnormal object       0.91      1.00      0.96        43\n",
      "\n",
      "       accuracy                           0.99      3065\n",
      "      macro avg       0.96      0.99      0.98      3065\n",
      "   weighted avg       0.99      0.99      0.99      3065\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labels, preds = evaluate_model(model, test_loader)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(labels, preds, target_names=[\"normal\", \"unusual action\", \"abnormal object\"]))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"normal\", \"unusual action\", \"abnormal object\"],\n",
    "            yticklabels=[\"normal\", \"unusual action\", \"abnormal object\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e5e5096-c218-445d-9cdd-c67765ed2068",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_avenue_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ba71ad4-6207-4d5e-9677-b6e780785b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/w1f1my553ll2rpzvjc_kqfx80000gn/T/ipykernel_80926/1833599731.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"final_avenue_model.pt\", map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnomalyClassifier(\n",
       "  (features): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Identity()\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AnomalyClassifier(num_classes=3).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"final_avenue_model.pt\", map_location=DEVICE))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fcec96-23af-4928-bf29-4a7c51da1ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16bd09d1-f3d3-4be9-8a6f-32baf9f63c5b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15c4857f-2d08-4e85-9fa5-71cbb10c6820",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL2IDX = {\"normal\": 0, \"unusual action\": 1, \"abnormal object\": 2}\n",
    "IDX2LABEL = {v: k for k, v in LABEL2IDX.items()}\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49050f68-00e6-4d8d-97e0-1ae5b9c5fd3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 92.1717, Accuracy: 88.66%\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \n",
    "                      \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "df = pd.read_csv(\"ave_anomaly_labels.csv\")\n",
    "df['label'] = df['anomaly_label'].map(LABEL2IDX)\n",
    "\n",
    "# Grouped train-test split by video\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(splitter.split(df, groups=df['video']))\n",
    "df_train, df_val = df.iloc[train_idx], df.iloc[val_idx]\n",
    "\n",
    "# Prepare datasets and loaders\n",
    "train_set = AvenueAnomalyDataset(df_train, \"datasets/Avenue Dataset/frames/test\", transform=transform)\n",
    "val_set = AvenueAnomalyDataset(df_val, \"datasets/Avenue Dataset/frames/test\", transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=32)\n",
    "\n",
    "# Initialize model\n",
    "model = AnomalyClassifier(num_classes=3).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = outputs.argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100 * correct / total\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Accuracy: {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c85c5fd8-71eb-41a7-9595-5c569c63fc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_op_avenue_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86467b58-9675-4bc1-a708-c8c40bfa9d32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s9/w1f1my553ll2rpzvjc_kqfx80000gn/T/ipykernel_70884/2235279122.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"final_op_avenue_model.pt\", map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AnomalyClassifier(\n",
       "  (base): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (3): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (4): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (5): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (2): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Sequential(\n",
       "      (0): Dropout(p=0.5, inplace=False)\n",
       "      (1): Linear(in_features=512, out_features=3, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AnomalyClassifier(num_classes=3).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"final_op_avenue_model.pt\", map_location=DEVICE))\n",
    "model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
